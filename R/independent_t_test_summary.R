#' @title Multiverse of Independent Samples t-test Results (Summary Statistics)
#'
#' @description
#' This function explores the "multiverse" of possible Cohen's *d* effect sizes,
#' confidence intervals (CIs), and *p*-values that are compatible with a set
#' of reported summary statistics (Means, SDs, Ns).
#'
#' The multiverse is generated by varying:
#' \itemize{
#'   \item \strong{Input Rounding/Grid generation}: Adjusting reported statistics 
#'         (Means and SDs) by creating a grid of potential "true" values within 
#'         the precision limits defined by \code{m_digits} and \code{sd_digits}.
#'         This relies on the \code{input_rounding} argument to determine if the 
#'         values should be treated as "rounded" (interval +/- 0.5 ULP) or 
#'         "truncated".
#'   \item \strong{Effect Size Type}: Computing both Cohen's *d* and Hedges' *g*.
#'   \item \strong{CI Methods}: Calculating Wald-type CIs (using *t* distributions 
#'         with pooled or Welch variance) and Noncentral *t* (NCT) distribution CIs.
#'   \item \strong{P-value Methods}: Using *t*-tests (Student's pooled or Welch's 
#'         unpooled).
#'   \item \strong{Output Rounding}: Applying different rounding methods 
#'         ("half_up", "half_down", "bankers", "trunc") to the calculated 
#'         *d*/CI and *p*-value to compare against the reported values.
#'   \item \strong{Direction}: The calculation uses the direction implied by the 
#'         order of the supplied means (M1 - M2) or its reverse, depending on 
#'         \code{direction}.
#'   \item \strong{SD vs. SE Interpretation (optional)}: If 
#'         \code{include_se_sd_confusion = TRUE}, the function includes a branch 
#'         where reported SDs are interpreted as standard errors and converted to 
#'         SDs via \eqn{SD = SE \times \sqrt{n}}.
#' }
#'
#' @param m1 Numeric. Mean of Group 1.
#' @param m2 Numeric. Mean of Group 2.
#' @param sd1 Numeric. Standard deviation of Group 1 (assumed sample SD by default).
#' @param sd2 Numeric. Standard deviation of Group 2 (assumed sample SD by default).
#' @param n1 Numeric. Sample size of Group 1.
#' @param n2 Numeric. Sample size of Group 2.
#' @param m_digits Integer. Number of decimal places to assume for the reported
#'   means (\code{m1}, \code{m2}). This controls the width of the interval within 
#'   which the "true" unrounded mean is assumed to lie.
#' @param sd_digits Integer. Number of decimal places to assume for the reported
#'   SDs (\code{sd1}, \code{sd2}). This controls the width of the interval within 
#'   which the "true" unrounded SD is assumed to lie.
#' @param input_rounding Character. How the input values (\code{m} and \code{sd}) 
#'   should be treated when generating the multiverse grid. Options are:
#'   \itemize{
#'     \item \code{"truncated"}: Assumes the reported values were truncated.
#'     \item \code{"rounded"}: Assumes the reported values were rounded 
#'           (typically +/- 0.5 unit in the last place).
#'   }
#'   Default is \code{"truncated"} if NULL.
#' @param output_rounding Character vector. Rounding methods applied to the 
#'   calculated results (effect sizes and p-values) to check for matches with 
#'   reported values. Allowed: \code{"half_up"}, \code{"half_down"}, 
#'   \code{"bankers"}, \code{"trunc"}. If \code{NULL}, all are used.
#' @param p Numeric. Reported p-value.
#' @param p_digits Integer. Number of decimal places to which the p-value was
#'   reported. Must be supplied if \code{p} is non-\code{NULL}.
#' @param p_methods Character vector. P-value methods to include.
#'   Allowed: \code{"student_t"}, \code{"welch_t"}.
#'   If \code{NULL}, both are used.
#' @param alpha Numeric. Significance level for CIs (default is 0.05).
#' @param d Numeric. Reported Cohen's *d* or Hedges' *g* estimate.
#' @param d_ci_lower Numeric. Reported lower bound of the CI for *d*.
#' @param d_ci_upper Numeric. Reported upper bound of the CI for *d*.
#' @param d_digits Integer. Number of decimal places to which *d* and its CI
#'   bounds were reported. Must be supplied if \code{d} is non-\code{NULL}.
#' @param direction Character. Whether to compute the effect as M1 - M2,
#'   M2 - M1, or both. One of \code{"m1_minus_m2"}, \code{"m2_minus_m1"},
#'   \code{"both"}.
#' @param ci_methods Character vector. CI methods to include.
#'   Allowed: \code{"wald_t"}, \code{"welch_t"}, \code{"nct"}.
#'   If \code{NULL}, all are used.
#' @param include_se_sd_confusion Logical. If \code{TRUE}, the multiverse also
#'   includes a branch that interprets \code{sd1} and \code{sd2} as standard
#'   errors and converts them to SDs via \eqn{SD = SE \times \sqrt{n}} before
#'   computing effect sizes, CIs, and p-values. Default is \code{FALSE}.
#'
#' @return A list with three elements:
#' \itemize{
#'   \item \code{reproduced}: Data frame summarising whether the reported *d*
#'         and *p* fall within the ranges generated by the multiverse (min/max 
#'         rounded values).
#'   \item \code{d_results}: Data frame detailing all calculated effect sizes and
#'         CIs. Contains additional columns such as \code{sd_interpretation},
#'         indicating whether SDs were used as reported (\code{"sd"}) or derived
#'         from SE (\code{"se_converted"}), and rounding indicators.
#'   \item \code{p_results}: Data frame detailing all calculated p-values.
#'         Includes \code{sd_interpretation}, \code{p_unrounded},
#'         \code{p_rounded}, and match indicators.
#' }
#'
#' @importFrom roundwork round_up round_down round_trunc
#' @importFrom stats pt qt uniroot pnorm qnorm
#' @importFrom dplyr summarise mutate arrange bind_cols select between
#' @importFrom tibble as_tibble rownames_to_column
#' @importFrom scales breaks_pretty
#' 
#' @examples
#' # --------------------------------------------------------------------------
#' # Example: Checking reported stats from a paper
#' # --------------------------------------------------------------------------
#' # Scenario: A paper reports:
#' #   Group 1: M = 10.30, SD = 3.12, N = 50
#' #   Group 2: M = 8.71,  SD = 2.80, N = 48
#' #   Results: t-test p = 0.009, Cohen's d = 0.53, 95% CI [0.20, 0.90]
#'
#' res <- independent_t_test_summary(
#'   # --- Essential Arguments ---
#'   m1 = 10.30,
#'   m2 = 8.71,
#'   sd1 = 3.12,
#'   sd2 = 2.80,
#'   n1 = 50,
#'   n2 = 48,
#'   # Define the precision of the input Means and SDs (decimal places)
#'   m_digits = 2,
#'   sd_digits = 2,
#'
#'   # --- Comparison Targets ---
#'   # Reported p-value
#'   p = 0.009,
#'   p_digits = 3,
#'
#'   # Reported Effect Size (Cohen's d) and CIs
#'   d = 0.53,
#'   d_ci_lower = 0.20,
#'   d_ci_upper = 0.90,
#'   d_digits = 2,
#'
#'   # --- Multiverse Configuration ---
#'   # Method settings (NULL = check all variants)
#'   p_methods = NULL,       # Check both Student's and Welch's
#'   ci_methods = NULL,      # Check Wald (Student/Welch) and NCT
#'   output_rounding = NULL, # Check all rounding methods (half_up, bankers, etc.)
#'   input_rounding = "truncated", # Assume inputs might be truncated or rounded
#'
#'   # Assumptions
#'   alpha = 0.05,
#'   direction = "m1_minus_m2",
#'   include_se_sd_confusion = FALSE
#' )
#'
#' # View the summary of reproduction
#' res$reproduced
#'
#' # Plots
#' plot_multiverse_p(res)
#' plot_multiverse_d(res)
#'
#' @export
independent_t_test_summary <- function(
    m1 = NULL, 
    m2 = NULL, 
    sd1 = NULL, 
    sd2 = NULL,
    n1 = NULL, 
    n2 = NULL,
    m_digits = NULL,
    sd_digits = NULL,
    input_rounding = NULL,
    output_rounding = NULL,
    p = NULL, 
    p_digits = NULL,
    p_methods = NULL,
    alpha = 0.05,
    d = NULL, 
    d_ci_lower = NULL, 
    d_ci_upper = NULL,
    d_digits = NULL,
    direction = c("m1_minus_m2", "m2_minus_m1", "both"),
    ci_methods = NULL,
    include_se_sd_confusion = FALSE
) {
  
  direction <- match.arg(direction)

  # Coerce reported values
  d_num          <- if (!is.null(d)) as.numeric(d) else NA_real_
  d_ci_lower_num <- if (!is.null(d_ci_lower)) as.numeric(d_ci_lower) else NA_real_
  d_ci_upper_num <- if (!is.null(d_ci_upper)) as.numeric(d_ci_upper) else NA_real_
  p_num          <- if (!is.null(p)) as.numeric(p) else NA_real_
  
  # --- Validation of m_digits and sd_digits (ULP for M and SD) ---------------
  # Literal precision of inputs as written (for diagnostics)
  obs_m_digits  <- max(.get_digits(m1),  .get_digits(m2))
  obs_sd_digits <- max(.get_digits(sd1), .get_digits(sd2))
  
  # Require m_digits
  if (is.null(m_digits)) {
    stop(
      "You must supply 'm_digits'. ",
      "Specify the number of decimal places the means (m1, m2) were reported to, ",
      "e.g. m_digits = 1 for 10.3 or m_digits = 2 for 10.30."
    )
  }
  # Require sd_digits
  if (is.null(sd_digits)) {
    stop(
      "You must supply 'sd_digits'. ",
      "Specify the number of decimal places the SDs (sd1, sd2) were reported to, ",
      "e.g. sd_digits = 2 for 3.10."
    )
  }
  
  # m_digits checks
  if (!is.numeric(m_digits) || length(m_digits) != 1L ||
      !is.finite(m_digits) || m_digits < 0) {
    stop("'m_digits' must be a single non-negative finite integer (e.g. 1 or 2).")
  }
  if (m_digits %% 1 != 0) {
    stop(
      "'m_digits' must be an integer number of decimal places (e.g. 1 or 2), not ",
      m_digits, "."
    )
  }
  m_digits <- as.integer(m_digits)
  
  if (m_digits < obs_m_digits) {
    stop(
      "m_digits = ", m_digits, " is smaller than the apparent decimal precision of m1/m2 (",
      obs_m_digits, "). This suggests that the means carry more decimal places than ",
      "your rounding grid assumes (often due to floating-point artefacts or prior ",
      "processing)."
    )
  }
  if (m_digits > obs_m_digits) {
    warning(
      "m_digits = ", m_digits, " is larger than the apparent precision of m1/m2 (",
      obs_m_digits, "). This assumes implied trailing zeros (e.g., stored 3.1 ",
      "representing a reported 3.10). Please verify that this matches the reporting."
    )
  }
  
  # sd_digits checks
  if (!is.numeric(sd_digits) || length(sd_digits) != 1L ||
      !is.finite(sd_digits) || sd_digits < 0) {
    stop("'sd_digits' must be a single non-negative finite integer (e.g. 2).")
  }
  if (sd_digits %% 1 != 0) {
    stop(
      "'sd_digits' must be an integer number of decimal places (e.g. 2), not ",
      sd_digits, "."
    )
  }
  sd_digits <- as.integer(sd_digits)
  
  if (sd_digits < obs_sd_digits) {
    stop(
      "sd_digits = ", sd_digits, " is smaller than the apparent decimal precision of sd1/sd2 (",
      obs_sd_digits, "). This suggests that the SDs carry more decimal places than ",
      "your rounding grid assumes (often due to floating-point artefacts)."
    )
  }
  if (sd_digits > obs_sd_digits) {
    warning(
      "sd_digits = ", sd_digits, " is larger than the apparent precision of sd1/sd2 (",
      obs_sd_digits, "). This assumes implied trailing zeros in the reported SDs ",
      "(e.g., 2.3 representing 2.30). Please verify that this matches the reporting."
    )
  }
  
  
  # If you report d, you must also tell us how many decimals it was rounded to
  if (!is.null(d) && is.null(d_digits)) {
    stop(
      "You supplied 'd' but not 'd_digits'. ",
      "Please provide the number of decimal places d was reported to, e.g. d_digits = 2."
    )
  }
  
  # If you report p, you must also tell us how many decimals it was rounded to
  if (!is.null(p) && is.null(p_digits)) {
    stop(
      "You supplied 'p' but not 'p_digits'. ",
      "Please provide the number of decimal places p was reported to, e.g. p_digits = 3."
    )
  }
  
  # Check that d_digits, if supplied, is a single non-negative integer
  if (!is.null(d_digits)) {
    if (!is.numeric(d_digits) || length(d_digits) != 1L || !is.finite(d_digits) || d_digits < 0) {
      stop("'d_digits' must be a single non-negative finite number (e.g. 2).")
    }
    d_digits <- as.integer(d_digits)
  } else {
    # No reported d: we still need some rounding for the multiverse output
    d_digits <- 2L
  }
  
  # Check that p_digits, if supplied, is a single non-negative integer
  if (!is.null(p_digits)) {
    if (!is.numeric(p_digits) || length(p_digits) != 1L || !is.finite(p_digits) || p_digits < 1) {
      stop("'p_digits' must be a single non-negative finite number greater than zero (e.g. 3).")
    }
    p_digits <- as.integer(p_digits)
  } else {
    # No reported p: we still need some rounding for the multiverse output
    p_digits <- 3L
  }
  
  if (is.null(input_rounding)) {
    input_rounding <- "truncated"
  }
  input_rounding <- match.arg(input_rounding, choices = c("truncated", "rounded"))
  
  if (!is.null(output_rounding)) {
    # This automatically checks validity and stops with an error if invalid
    output_rounding <- match.arg(
      output_rounding, 
      choices = c("half_up", "half_down", "bankers", "trunc"), 
      several.ok = TRUE
    )
  }
  
  params <- .multiverse_validate_and_setup_summary(
    ci_methods, p_methods, input_rounding, output_rounding,
    m1, m2, sd1, sd2, n1, n2
  )
  
  d_results <- list()
  p_results <- list()
  idx_d <- 1
  idx_p <- 1
  
  res_summary <- .multiverse_from_summary_stats(
    m1, m2, sd1, sd2, n1, n2,
    params$ci_methods, params$p_methods, 
    input_rounding = input_rounding, params$output_rounding,
    d_digits, p_digits, m_digits, sd_digits, alpha, 
    d_num, d_ci_lower_num, d_ci_upper_num, p_num,
    idx_d, idx_p,
    direction = direction,
    include_se_sd_confusion = include_se_sd_confusion
  )
  d_results <- c(d_results, res_summary$d_results)
  p_results <- c(p_results, res_summary$p_results)
  
  d_out <- if (length(d_results) > 0) do.call(rbind, d_results) else NULL
  p_out <- if (length(p_results) > 0) do.call(rbind, p_results) else NULL
  
  if (!is.null(d_out)) {
    d_out <- d_out[order(!d_out$match_all,
                         !d_out$match_est,
                         !d_out$match_ci_lower,
                         !d_out$match_ci_upper), ]
  }
  
  reproduced_out <- {
    # ---- Summaries of multiverse d-values -------------------------------------
    d_summary <- if (!is.null(d_out) && nrow(d_out) > 0 &&
                     "d_rounded" %in% names(d_out)) {
      d_out |>
        dplyr::summarise(
          min_d_rounded = min(d_rounded, na.rm = TRUE),
          max_d_rounded = max(d_rounded, na.rm = TRUE)
        )
    } else {
      tibble::tibble(
        min_d_rounded = NA_real_,
        max_d_rounded = NA_real_
      )
    }
    
    # ---- Summaries of multiverse p-values -------------------------------------
    p_summary <- if (!is.null(p_out) && nrow(p_out) > 0 &&
                     "p_rounded" %in% names(p_out)) {
      p_out |>
        dplyr::summarise(
          min_p_rounded = min(p_rounded, na.rm = TRUE),
          max_p_rounded = max(p_rounded, na.rm = TRUE)
        )
    } else {
      tibble::tibble(
        min_p_rounded = NA_real_,
        max_p_rounded = NA_real_
      )
    }
    
    # ---- Reported values (may be NULL) ----------------------------------------
    reported_d <- if (exists("d") && !is.null(d)) d else NA_real_
    reported_p <- if (exists("p") && !is.null(p)) p else NA_real_
    
    combined <- dplyr::bind_cols(
      d_summary,
      p_summary,
      tibble::tibble(d = reported_d, p = reported_p)
    )
    
    # ---- Logical checks (NULL-safe) -------------------------------------------
    combined <- combined |>
      dplyr::mutate(
        d_inbounds = ifelse(
          is.na(d),
          NA,
          dplyr::between(d, min_d_rounded, max_d_rounded)
        ),
        p_inbounds = ifelse(
          is.na(p),
          NA,
          dplyr::between(p, min_p_rounded, max_p_rounded)
        )
      )
    
    combined |>
      dplyr::mutate(d = d,
                    p = p) |>
      dplyr::select(
        d, min_d_rounded, max_d_rounded, d_inbounds,
        p, min_p_rounded, max_p_rounded, p_inbounds
      )
  }
  
  list(
    reproduced = reproduced_out,
    d_results  = d_out,
    p_results  = p_out
  )
}

# --- Internal Helper Functions ---

#' @keywords internal
.hedges_correction <- function(d, df_s) {
  J_s   <- 1 - 3 / (4 * df_s - 1)
  g_raw <- J_s * d
  g_raw
}

#' @keywords internal
.calculate_cohen_d_stats <- function(m1, m2, sd1, sd2, n1, n2) {
  # Mean diff
  diff_mean <- m1 - m2
  
  # Pooled SD and DF (needed for student t and d)
  df_s <- n1 + n2 - 2
  sp   <- sqrt(((n1 - 1) * sd1^2 + (n2 - 1) * sd2^2) / df_s)
  
  # Raw d
  d_raw <- diff_mean / sp
  
  # Return everything needed for downstream calculations
  list(
    d_raw = d_raw,
    sp = sp,
    df_s = df_s,
    diff_mean = diff_mean
  )
}

#' @keywords internal
.calculate_student_t <- function(diff_mean, sp, n1, n2, df_s) {
  # Standard Error
  se_pooled <- sp * sqrt(1 / n1 + 1 / n2)
  t_val     <- diff_mean / se_pooled
  p_val     <- 2 * (1 - pt(abs(t_val), df = df_s))
  
  list(t = t_val, df = df_s, p = p_val, se = se_pooled)
}

#' @keywords internal
.calculate_welch_t <- function(m1, m2, sd1, sd2, n1, n2) {
  diff_mean <- m1 - m2
  var1 <- sd1^2
  var2 <- sd2^2
  
  # Welch SE and DF
  se_welch <- sqrt(var1 / n1 + var2 / n2)
  
  num_w <- (var1 / n1 + var2 / n2)^2
  den_w <- (var1^2 / (n1^2 * (n1 - 1))) + (var2^2 / (n2^2 * (n2 - 1)))
  df_w  <- num_w / den_w
  
  t_val <- diff_mean / se_welch
  p_val <- 2 * (1 - pt(abs(t_val), df = df_w))
  
  list(t = t_val, df = df_w, p = p_val, se = se_welch)
}

#' @keywords internal
.calculate_es_ci <- function(es_est, es_type, ci_method, 
                             t_val, df, n1, n2, alpha, 
                             se_pooled_es, se_welch_es = NA) {
  
  # 1. Non-central t (NCT)
  if (ci_method == "nct") {
    # .nct_ci is your existing root-finding helper
    delta_ci <- .nct_ci(t_val, df, alpha = alpha) 
    
    if (any(is.na(delta_ci))) return(c(lower = NA_real_, upper = NA_real_))
    
    fac_d <- sqrt(1 / n1 + 1 / n2)
    dL    <- delta_ci[1] * fac_d
    dU    <- delta_ci[2] * fac_d
    
    # Apply Hedges correction to bounds if necessary
    if (es_type == "g") {
      J_s <- 1 - 3 / (4 * df - 1)
      dL  <- dL * J_s
      dU  <- dU * J_s
    }
    return(c(lower = dL, upper = dU))
  } 
  
  # 2. Central t methods (Wald / Welch)
  # Determine which SE to use for the interval width
  se_use <- if (ci_method == "welch_t" && !is.na(se_welch_es)) se_welch_es else se_pooled_es
  
  crit <- qt(1 - alpha / 2, df = df)
  
  ci_lower <- es_est - crit * se_use
  ci_upper <- es_est + crit * se_use
  
  return(c(lower = ci_lower, upper = ci_upper))
}

#' @keywords internal
.adjust_value <- function(x, step, code) {
  if (step == 0) return(x)
  if (code == "minus") return(x - step)
  if (code == "plus")  return(x + step)
  x  # "reported"
}

#' @keywords internal
.nct_ci <- function(t_obs, df, alpha = 0.05, max_ncp = 1000) {
  suppress_nct_warning <- function(expr) {
    withCallingHandlers(
      expr,
      warning = function(w) {
        if (grepl("full precision may not have been achieved in 'pnt\\{final\\}'",
                  conditionMessage(w))) {
          invokeRestart("muffleWarning")
        }
      }
    )
  }
  
  fL <- function(delta) suppress_nct_warning(pt(t_obs, df = df, ncp = delta) - alpha / 2)
  fU <- function(delta) suppress_nct_warning(pt(t_obs, df = df, ncp = delta, lower.tail = FALSE) - alpha / 2)
  
  lower <- -max_ncp
  upper <- max_ncp
  
  fL_low <- fL(lower)
  fL_high <- fL(upper)
  if (is.na(fL_low) || is.na(fL_high) || fL_low * fL_high > 0) {
    delta_L <- NA_real_
  } else {
    delta_L <- tryCatch(uniroot(fL, lower = lower, upper = upper)$root, error = function(e) NA_real_)
  }
  
  fU_low_check <- fU(upper)
  fU_high_check <- fU(lower)
  if (is.na(fU_low_check) || is.na(fU_high_check) || fU_low_check * fU_high_check > 0) {
    fU_low_rev <- fU(lower)
    fU_high_rev <- fU(upper)
    if (is.na(fU_low_rev) || is.na(fU_high_rev) || fU_low_rev * fU_high_rev > 0) {
      delta_U <- NA_real_
    } else {
      delta_U <- tryCatch(uniroot(fU, lower = lower, upper = upper)$root, error = function(e) NA_real_)
    }
  } else {
    delta_U <- tryCatch(uniroot(fU, lower = lower, upper = upper)$root, error = function(e) NA_real_)
  }
  
  c(delta_L, delta_U)
}

#' @keywords internal
.multiverse_validate_and_setup_summary <- function(
    ci_methods, p_methods, input_rounding, output_rounding,
    m1, m2, sd1, sd2, n1, n2
) {
  allowed_ci_methods      <- c("wald_t", "welch_t", "nct")
  allowed_p_methods       <- c("student_t", "welch_t")
  allowed_output_rounding <- c("half_up", "half_down", "bankers", "trunc")
  
  check_methods <- function(input, allowed, name) {
    if (!is.null(input)) {
      bad <- setdiff(input, allowed)
      if (length(bad) > 0L) {
        stop(
          name, " contains invalid values: ",
          paste0(bad, collapse = ", "),
          "\nAllowed: ", paste0(allowed, collapse = ", ")
        )
      }
      return(input)
    }
    allowed
  }
  
  ci_methods      <- check_methods(ci_methods,      allowed_ci_methods,      "ci_methods")
  p_methods       <- check_methods(p_methods,       allowed_p_methods,       "p_methods")
  output_rounding <- check_methods(output_rounding, allowed_output_rounding, "output_rounding")
  
  vals_summary <- list(m1, m2, sd1, sd2, n1, n2)
  have_summary <- all(vapply(
    vals_summary,
    function(z) !is.null(z) && length(z) > 0 && !is.na(z[1L]),
    logical(1)
  ))
  
  if (!have_summary) {
    stop("Provide M/SD/N: m1, m2, sd1, sd2, n1, n2.")
  }
  
  list(
    ci_methods      = ci_methods,
    p_methods       = p_methods,
    input_rounding  = input_rounding,
    output_rounding = output_rounding
  )
}

#' @keywords internal
.get_digits <- function(x) {
  if (is.null(x) || length(x) == 0 || is.na(x)) return(0L)
  sx <- sub("^-", "", as.character(x[1L]))
  if (!grepl("\\.", sx)) return(0L)
  nchar(sub("^[^.]*\\.", "", sx))
}

#' @keywords internal
.multiverse_from_summary_stats <- function(
    m1, m2, sd1, sd2, n1, n2,
    ci_methods, p_methods, input_rounding, output_rounding,
    d_digits, p_digits, m_digits, sd_digits, alpha,
    d_num, d_ci_lower_num, d_ci_upper_num, p_num,
    idx_d, idx_p,
    direction = c("m1_minus_m2", "m2_minus_m1", "both"),
    include_se_sd_confusion = FALSE
) {
  d_results <- list()
  p_results <- list()
  
  direction <- match.arg(direction)
  dir_modes <- if (direction == "both") c("m1_minus_m2", "m2_minus_m1") else direction
  
  # Step sizes
  multiplier <- if (input_rounding == "truncated"){
    1.0
  } else if(input_rounding == "rounded"){
    0.5
  }
  
  step_m1  <- multiplier * 10^(-m_digits)
  step_m2  <- multiplier * 10^(-m_digits)
  step_sd1 <- multiplier * 10^(-sd_digits)
  step_sd2 <- multiplier * 10^(-sd_digits)
  
  # Define the grid of adjustments.
  # Instead of one "adj_stats" applied to all, we generate combinations.
  # To save compute time, we don't need every permutation of Reported/Plus/Minus.
  # We mostly need the extremes. However, a full grid for 4 vars is only 
  # 3^4 = 81 iterations, which is computationally trivial.
  
  adj_levels <- c("reported", "minus", "plus")
  
  # Create a grid of all possible adjustment combinations
  # We use integer indices: 1=reported, 2=minus, 3=plus
  grid_indices <- expand.grid(
    m1_idx  = 1:3,
    m2_idx  = 1:3,
    sd1_idx = 1:3,
    sd2_idx = 1:3
  )
  
  sd_modes  <- c("sd")
  if (isTRUE(include_se_sd_confusion)) {
    sd_modes <- c(sd_modes, "se_converted")
  }
  
  # Pre-calculate adjusted values to avoid re-calc inside loop
  m1_vals  <- c(m1, m1 - step_m1, m1 + step_m1)
  m2_vals  <- c(m2, m2 - step_m2, m2 + step_m2)
  sd1_vals <- c(sd1, sd1 - step_sd1, sd1 + step_sd1)
  sd2_vals <- c(sd2, sd2 - step_sd2, sd2 + step_sd2)
  
  # Iterate through the grid
  for (i in seq_len(nrow(grid_indices))) {
    
    # Retrieve adjusted values based on grid indices
    row <- grid_indices[i, ]
    
    m1_code  <- adj_levels[row$m1_idx]
    m2_code  <- adj_levels[row$m2_idx]
    sd1_code <- adj_levels[row$sd1_idx]
    sd2_code <- adj_levels[row$sd2_idx]
    
    m1_star  <- .adjust_value(m1,  step_m1,  m1_code)
    m2_star  <- .adjust_value(m2,  step_m2,  m2_code)
    sd1_star <- .adjust_value(sd1, step_sd1, sd1_code)
    sd2_star <- .adjust_value(sd2, step_sd2, sd2_code)
    
    adj_stats_label <- paste0(
      "m1:",  m1_code,  "|",
      "m2:",  m2_code,  "|",
      "sd1:", sd1_code, "|",
      "sd2:", sd2_code
    )
    
    for (sd_mode in sd_modes) {
      
      if (sd_mode == "sd") {
        sd1_eff <- sd1_star
        sd2_eff <- sd2_star
      } else { 
        sd1_eff <- sd1_star * sqrt(n1)
        sd2_eff <- sd2_star * sqrt(n2)
      }
      
      if (sd1_eff <= 0 || sd2_eff <= 0) next
      
      df_s <- n1 + n2 - 2
      N    <- n1 + n2
      
      # Pooled SD
      sp <- sqrt(((n1 - 1) * sd1_eff^2 + (n2 - 1) * sd2_eff^2) / df_s)
      if (!is.finite(sp) || sp <= 0) next
      
      var1 <- sd1_eff^2
      var2 <- sd2_eff^2
      se_welch_base <- sqrt(var1 / n1 + var2 / n2)
      
      num_w <- (var1 / n1 + var2 / n2)^2
      den_w <- (var1^2 / (n1^2 * (n1 - 1))) + (var2^2 / (n2^2 * (n2 - 1)))
      df_w <- num_w / den_w
      if (!is.finite(df_w) || df_w <= 0) df_w <- NA_real_
      
      # ----------------------------------------------------------
      # Loop over direction modes
      # ----------------------------------------------------------
      for (sd_mode in sd_modes) {
        
        # Handle SD interpretation
        if (sd_mode == "sd") {
          sd1_eff <- sd1_star
          sd2_eff <- sd2_star
        } else { 
          sd1_eff <- sd1_star * sqrt(n1)
          sd2_eff <- sd2_star * sqrt(n2)
        }
        
        if (sd1_eff <= 0 || sd2_eff <= 0) next
        
        # 1. Calculate Base Stats (Pooled) - Reused for d, g, and student t
        stats_base <- .calculate_cohen_d_stats(m1_star, m2_star, sd1_eff, sd2_eff, n1, n2)
        
        # 2. Calculate Welch Stats
        stats_welch <- .calculate_welch_t(m1_star, m2_star, sd1_eff, sd2_eff, n1, n2)
        
        # Loop over direction modes
        for (dir_mode in dir_modes) {
          
          sign_factor     <- if (dir_mode == "m1_minus_m2") 1 else -1
          direction_label <- dir_mode
          
          # Adjust effect direction
          # Note: stats_base$d_raw assumes m1-m2. We flip if needed.
          d_raw_directed <- stats_base$d_raw * sign_factor
          diff_mean_dir  <- stats_base$diff_mean * sign_factor
          
          # 3. Calculate Student t (using directed mean diff)
          res_student <- .calculate_student_t(diff_mean_dir, stats_base$sp, n1, n2, stats_base$df_s)
          
          # 4. Calculate Hedges g
          g_raw_directed <- .hedges_correction(d_raw_directed, stats_base$df_s)
          
          # ----------------------------------------------------------
          # Effect sizes + CIs
          # ----------------------------------------------------------
          for (es_type in c("d", "g")) {
            
            es_est <- if (es_type == "d") d_raw_directed else g_raw_directed
            N      <- n1 + n2
            
            # Pre-calculate SE of the Effect Size (needed for Wald/Welch CIs)
            # Note: This is SE of *d*, not SE of the mean difference.
            se_pooled_es <- sqrt(N / (n1 * n2) + es_est^2 / (2 * stats_base$df_s))
            se_welch_es  <- if (!is.na(stats_welch$df)) sqrt(N / (n1 * n2) + es_est^2 / (2 * stats_welch$df)) else NA
            
            for (ci_method in ci_methods) {
              
              # Select T and DF based on method
              # If using Wald/Student, we use pooled T. If Welch, we use Welch T.
              if (ci_method == "welch_t") {
                t_use  <- stats_welch$t * sign_factor # Apply direction
                df_use <- stats_welch$df
              } else {
                t_use  <- res_student$t # Already directed
                df_use <- res_student$df
              }
              
              # Call new CI Helper
              ci_vals <- .calculate_es_ci(
                es_est, es_type, ci_method, 
                t_use, df_use, n1, n2, alpha,
                se_pooled_es, se_welch_es
              )
              
              if (is.na(ci_vals['lower'])) next
              
              # Rounding Loop
              for (d_rounding in output_rounding) {
                
                d_round_fun <- switch(d_rounding,
                                      "half_up"   = function(x) roundwork::round_up(x,   d_digits),
                                      "half_down" = function(x) roundwork::round_down(x, d_digits),
                                      "bankers"   = function(x) round(x, d_digits),
                                      "trunc"     = function(x) roundwork::round_trunc(x, d_digits)
                )
                
                est_d   <- d_round_fun(es_est)
                lower_d <- d_round_fun(ci_vals['lower'])
                upper_d <- d_round_fun(ci_vals['upper'])
                
                # Save Results
                d_results[[idx_d]] <- data.frame(
                  source              = "summary",
                  direction           = direction_label,
                  es_type             = es_type,
                  ci_method           = ci_method,
                  d_rounding          = d_rounding,
                  input_adj_stats     = adj_stats_label,
                  sd_interpretation   = sd_mode,
                  # Store Used Stats from our objects
                  t_used              = t_use,
                  df_used             = df_use,
                  d_unrounded         = es_est,
                  ci_lower_unrounded  = ci_vals['lower'],
                  ci_upper_unrounded  = ci_vals['upper'],
                  d_rounded           = est_d,
                  ci_lower_rounded    = lower_d,
                  ci_upper_rounded    = upper_d,
                  # [Matching logic remains the same...]
                  match_est      = if (!is.na(d_num))          isTRUE(all.equal(est_d,   d_num))          else NA,
                  match_ci_lower = if (!is.na(d_ci_lower_num)) isTRUE(all.equal(lower_d, d_ci_lower_num)) else NA,
                  match_ci_upper = if (!is.na(d_ci_upper_num)) isTRUE(all.equal(upper_d, d_ci_upper_num)) else NA,
                  match_all      = if (!any(is.na(c(d_num, d_ci_lower_num, d_ci_upper_num)))) {
                    est_d   == d_num &&
                      lower_d == d_ci_lower_num &&
                      upper_d == d_ci_upper_num
                  } else NA,
                  stringsAsFactors = FALSE
                )
                idx_d <- idx_d + 1
              }
            }
          }
          
          # ----------------------------------------------------------
          # p-values 
          # ----------------------------------------------------------
          for (p_method in p_methods) {
            
            # Select result object based on method
            res_obj <- if(p_method == "student_t") res_student else stats_welch
            
            if (is.na(res_obj$t) || is.na(res_obj$df)) next
            
            p_unr <- res_obj$p
            
            for (p_rounding in output_rounding) {
              
              p_round_fun <- switch(p_rounding,
                                    "half_up"   = function(x) roundwork::round_up(x,   p_digits),
                                    "half_down" = function(x) roundwork::round_down(x, p_digits),
                                    "bankers"   = function(x) round(x, p_digits),
                                    "trunc"     = function(x) roundwork::round_trunc(x, p_digits)
              )
              
              p_rounded <- p_round_fun(p_unr)
              
              p_results[[idx_p]] <- data.frame(
                source            = "summary",
                direction         = direction_label,
                p_method          = p_method,
                p_rounding        = p_rounding,
                input_adj_stats   = adj_stats_label,
                sd_interpretation = sd_mode,
                t_used            = res_obj$t * sign_factor, # Apply direction to T
                df_used           = res_obj$df,
                p_unrounded       = p_unr,
                p_rounded         = p_rounded,
                match_p           = if (!is.na(p_num)) isTRUE(all.equal(p_rounded, p_num)) else NA,
                stringsAsFactors  = FALSE
              )
              idx_p <- idx_p + 1
            }
          }
        } 
      }
    }
  }
  
  list(d_results = d_results, p_results = p_results, idx_d = idx_d, idx_p = idx_p)
}

#' Plot multiverse of Cohen's d results from independent_t_test_summary
#'
#' @param res A list returned by independent_t_test_summary(),
#'   containing an element `d_results` with columns:
#'   `d_rounded`, `ci_lower_rounded`, and `ci_upper_rounded`.
#'
#' @return A ggplot object visualizing the range of recalculated rounded
#'   Cohen's d and its 95% confidence intervals across all analytic choices.
#'
#' @examples
#' \dontrun{
#' plot_multiverse_d(res)
#' }
#' 
#' @export
plot_multiverse_d <- function(res) {
  if (is.null(res$d_results) || nrow(res$d_results) == 0) {
    stop("`res` must include a non-empty `d_results` data frame.")
  }
  
  res_d <- res$d_results |>
    tibble::as_tibble() |>
    dplyr::arrange(.data$d_rounded) |>
    tibble::rownames_to_column(var = "rowname") |>
    dplyr::mutate(rowname = as.numeric(.data$rowname))
  
  # Define shaded regions based on range of CI limits and point estimates
  xmin_shade  <- min(res_d$ci_lower_rounded, na.rm = TRUE)
  xmax_shade  <- max(res_d$ci_upper_rounded, na.rm = TRUE)
  xmin_shade2 <- min(res_d$d_rounded, na.rm = TRUE)
  xmax_shade2 <- max(res_d$d_rounded, na.rm = TRUE)
  
  # Construct the plot
  ggplot2::ggplot(
    res_d,
    ggplot2::aes(
      y = .data$rowname,
      x = .data$d_rounded,
      xmin = .data$ci_lower_rounded,
      xmax = .data$ci_upper_rounded
    )
  ) +
    ggplot2::annotate(
      "rect",
      xmin = xmin_shade,
      xmax = xmax_shade,
      ymin = -Inf,
      ymax = Inf,
      alpha = 0.3
    ) +
    ggplot2::annotate(
      "rect",
      xmin = xmin_shade2,
      xmax = xmax_shade2,
      ymin = -Inf,
      ymax = Inf,
      alpha = 0.4
    ) +
    ggstance::geom_linerangeh() +
    ggplot2::geom_point() +
    ggplot2::theme_linedraw() +
    ggplot2::scale_x_continuous(
      name = "Recalculated rounded Cohen's d and 95% CIs",
      breaks = scales::breaks_pretty(n = 8),
      expand = ggplot2::expansion(mult = c(0.1, 0.1))
    ) +
    ggplot2::scale_y_continuous(
      labels = NULL,
      name = "Set of analytic choices"
    ) +
    ggplot2::ggtitle("Multiverse of Cohen's d and its 95% CIs")
}

#' Plot multiverse of p-values from independent_t_test_summary
#'
#' @param res A list returned by independent_t_test_summary(),
#'   containing an element `p_results` with column `p_unrounded`.
#'
#' @return A ggplot object visualizing the distribution of recalculated
#'   unrounded p-values across all analytic choices.
#'
#' @examples
#' \dontrun{
#' plot_multiverse_p(res)
#' }
#' 
#' @export
plot_multiverse_p <- function(res) {
  if (is.null(res$p_results) || nrow(res$p_results) == 0) {
    stop("`res` must include a non-empty `p_results` data frame.")
  }
  
  res_p <- res$p_results |>
    tibble::as_tibble() |>
    dplyr::arrange(.data$p_unrounded) |>
    tibble::rownames_to_column(var = "rowname") |>
    dplyr::mutate(rowname = as.numeric(.data$rowname))
  
  # Shaded region showing full range of p-values
  xmin_shade <- min(res_p$p_unrounded, na.rm = TRUE)
  xmax_shade <- max(res_p$p_unrounded, na.rm = TRUE)
  
  # Build the plot
  ggplot2::ggplot(
    res_p,
    ggplot2::aes(
      y = .data$rowname,
      x = .data$p_unrounded
    )
  ) +
    ggplot2::annotate(
      "rect",
      xmin = xmin_shade,
      xmax = xmax_shade,
      ymin = -Inf,
      ymax = Inf,
      alpha = 0.4
    ) +
    ggplot2::geom_point() +
    ggplot2::theme_linedraw() +
    ggplot2::scale_x_continuous(
      name = "Recalculated unrounded p-value",
      breaks = scales::breaks_pretty(n = 8),
      expand = ggplot2::expansion(mult = c(0.1, 0.1))
    ) +
    ggplot2::scale_y_continuous(
      labels = NULL,
      name = "Set of analytic choices"
    ) +
    ggplot2::ggtitle("Multiverse of p-values")
}